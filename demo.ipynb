{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_dataset import VideoFrameDataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        # T.Normalize(\n",
    "        #     mean=(0.485, 0.456, 0.406),\n",
    "        #     std=(0.229, 0.224, 0.225)\n",
    "        # ),\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# random.seed(0)\n",
    "root = os.path.join(os.getcwd(), 'zaloai/train/videos')  # Folder in which all videos lie in a specific structure\n",
    "annotation_file = os.path.join(root.replace('videos', ''), 'annotations.txt')  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "\"\"\" DEMO 1 WITHOUT IMAGE TRANSFORMS \"\"\"\n",
    "dataset = VideoFrameDataset(\n",
    "    root_path=root,\n",
    "    annotationfile_path=annotation_file,\n",
    "    num_segments=16,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=val_transform,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "sample = dataset[random.randint(0, 351)]  # take first sample of dataset \n",
    "frames = sample[0]   # list of PIL images\n",
    "label = sample[1]    # integer label\n",
    "path = sample[2]\n",
    "print(frames.shape)\n",
    "print(path)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/vietlq4/zaloaichallenge/zaloai/train/label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['fname'], df['liveness_score'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [train.replace('.mp4','') for train in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [test.replace('.mp4', '') for test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = '/home/vietlq4/zaloaichallenge/zaloai/train/videos'\n",
    "annotation_path = '/home/vietlq4/zaloaichallenge/zaloai/train/label.csv'\n",
    "annotation_out_file = '/home/vietlq4/zaloaichallenge/zaloai/train/train_annotations.txt'\n",
    "\n",
    "# video_filenames = os.listdir(path)\n",
    "label_df = None\n",
    "label2id = {}\n",
    "if len(annotation_path) != 0:\n",
    "    label_df = pd.read_csv(annotation_path)\n",
    "\n",
    "    for idx, row in label_df.iterrows():\n",
    "        label2id[row['fname'].replace('.mp4', '')] = int(row['liveness_score'])\n",
    "\n",
    "with open(annotation_out_file, 'w') as f:\n",
    "\n",
    "    for video_filename in sorted(X_train, key= lambda x: int(x)):\n",
    "        start_frame = 0\n",
    "        video_path = os.path.join(path, video_filename)\n",
    "        try:\n",
    "            num_frames = len(os.listdir(video_path))\n",
    "            if num_frames == 0:\n",
    "                print(f'{video_filename}- no frames')\n",
    "                continue\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        if len(annotation_path) == 0:\n",
    "            annotation_string = \"{} {} {}\\n\".format(\n",
    "                video_filename, start_frame, num_frames - 1)\n",
    "        else:\n",
    "            annotation_string = \"{} {} {} {}\\n\".format(\n",
    "                video_filename, start_frame, num_frames - 1, label2id[video_filename])\n",
    "\n",
    "        f.write(annotation_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(target)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v!r}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn \n",
    "from typing import Optional, Sequence\n",
    "from torch import Tensor\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha: Optional[Tensor] = 0.25,\n",
    "                 gamma: float = 2,\n",
    "                 reduction: str = 'mean',):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, x, y):\n",
    "        loss = torchvision.ops.sigmoid_focal_loss(input, target, alpha=self.alpha, gamma=self.gamma,reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-5, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets *\n",
    "                       torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                with torch.no_grad():\n",
    "                    # if self.disable_torch_grad_focal_loss:\n",
    "                    #     torch._C.set_grad_enabled(False)\n",
    "                    self.xs_pos = self.xs_pos * self.targets\n",
    "                    self.xs_neg = self.xs_neg * self.anti_targets\n",
    "                    self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                                  self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "                    # if self.disable_torch_grad_focal_loss:\n",
    "                    #     torch._C.set_grad_enabled(True)\n",
    "                self.loss *= self.asymmetric_w\n",
    "            else:\n",
    "                self.xs_pos = self.xs_pos * self.targets\n",
    "                self.xs_neg = self.xs_neg * self.anti_targets\n",
    "                self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                              self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "                self.loss *= self.asymmetric_w\n",
    "        _loss = - self.loss.sum() / x.size(0)\n",
    "        _loss = _loss / y.size(1) * 1000\n",
    "\n",
    "        return _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "input = torch.rand(4,1, requires_grad=True)\n",
    "target = torch.randint(0, 1, (4,1)).random_(2).type(torch.FloatTensor)\n",
    "print(target)\n",
    "loss = AsymmetricLossOptimized()\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/vietlq4/zaloaichallenge/zaloai/public_test/videos/944/img_00000.jpg'\n",
    "img = cv2.imread('/home/vietlq4/zaloaichallenge/zaloai/public_test/videos/944/img_00000.jpg')\n",
    "img_det = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_videos = os.listdir('/home/vietlq4/zaloaichallenge/face/train/videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in list_videos:\n",
    "    if len(os.listdir(os.path.join('/home/vietlq4/zaloaichallenge/face/train/videos', video))) < 4:\n",
    "        print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdMSoftmaxLoss(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s= 30., m=0.4):\n",
    "        super(AdMSoftmaxLoss, self).__init__()\n",
    "        self.s = s \n",
    "        self.m = m \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(self.in_features, self.out_features,bias=False)\n",
    "        \n",
    "    def forward(self, x,labels):\n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, dim=1)\n",
    "\n",
    "        x = F.normalize(x, dim=1)\n",
    "        \n",
    "        wf = self.fc(x)\n",
    "        \n",
    "        numerator = self.s *(torch.diagonal(wf.transpose(0,1)[labels]) - self.m)\n",
    "        excl = torch.cat([torch.cat((wf[i,:y],wf[i,y+1:])).unsqueeze(0) for i,y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s*excl), dim =1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vietlq4/miniconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(4,2, requires_grad=True)\n",
    "target = torch.randint(0, 1, (4,)).random_(2).type(torch.LongTensor)\n",
    "feature_input = torch.rand(4,14,14,requires_grad=True)\n",
    "feature_input = F.sigmoid(feature_input)\n",
    "feature_target = torch.randint(0, 2, (4,14,14)).random_(1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print(feature_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.CrossEntropyLoss()\n",
    "l2 = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l1(input,target) + l2(feature_input,feature_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.ones(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.abs(torch.sub(label, 0.01)).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9900],\n",
       "        [0.9900],\n",
       "        [0.9900],\n",
       "        [0.9900]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(4, 196)\n",
    "label_map = ones * labels.expand_as(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c844a1953941c6e994ad36ecffaa47cd6e7c29c36adf75c89300a3339db050f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
